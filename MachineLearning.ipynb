{
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "e9ebdef7-a3e4-46ee-892c-c70e55c34cae",
        "_execution_state": "idle",
        "_uuid": "93730b266672fdd1311b81c481e981c30777d582"
      },
      "source": "*Statistics Notes*\n------------------------------------------\n\n\n----------\n",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "d51e50dd796d18c57896061c52d6b73744837191",
        "_cell_guid": "aea9c6b1-0dd6-4c9a-b132-288b2a1b3ee5",
        "_execution_state": "idle",
        "trusted": false
      },
      "source": "import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom   sklearn import datasets, linear_model\nfrom   IPython.display import display, Math, Latex\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "5901b1b9-3786-4e0f-a9f2-7d64772bb6e6",
        "_execution_state": "idle",
        "_uuid": "880ae3ae8aaed7d89d2b0612383ab1c6f4b2caba"
      },
      "source": "**Descriptive statistics** are numbers that are used to summarize and describe data. Descriptive statistics are just descriptive. They do not involve generalizing beyond the data at hand. Generalizing from our data to another set of cases is the business of **Inferential statistics**. Example of descriptive stat is average salary of Indians in year 2017.\n\nIn statistics, we often rely on a **sample** --- that is, a small subset of a larger set of data --- to draw inferences about the larger set. The larger set is known as the **population** from which the sample is drawn.\nThe mathematical procedures whereby we convert information about the sample into intelligent guesses about the population fall under the rubric of inferential statistics.\n\n**Sampling Strategies:**<br>\n&emsp; Researchers adopt a variety of sampling strategies. The most straightforward is **simple random sampling**. Such sampling requires every member of the population to have an equal chance of being selected into the sample. In addition, the selection of one member must be independent of the selection of every other member. That is, picking one member from the population must not increase or decrease the probability of picking any other member (relative to the others). In this sense, we can say that simple random sampling chooses a sample by pure chance.<br>\n&emsp; Since simple random sampling often does not ensure a representative sample, a sampling method called **stratified random sampling** is sometimes used to make the sample more representative of the population. This method can be used if the population has a number of distinct \"strata\" or groups. In stratified sampling, you first identify members of your sample who belong to each group. Then you randomly sample from each of those subgroups in such a way that the sizes of the subgroups in the sample are proportional to their sizes in the population.<br>\n&emsp; Let's take an example: Suppose you were interested in views of capital punishment at an urban university. You have the time and resources to interview 200 students. The student body is diverse with respect to age; many older people work during the day and enroll in night courses (average age is 39), while younger students generally enroll in day classes (average age of 19). It is possible that night students have different views about capital punishment than day students. If 70% of the students were day students, it makes sense to ensure that 70% of the sample consisted of day students. Thus, your sample of 200 students would consist of 140 day students and 60 night students. The proportion of day students in the sample and in the population (the entire university) would be the same. Inferences to the entire population of students at the university would therefore be more secure.\n\n**Percentiles:**<br>\n&emsp; A test score in and of itself is usually difficult to interpret. For example, if you learned that your score on a measure of shyness was 35 out of a possible 50, you would have little idea how shy you are compared to other people. More relevant is the percentage of people with lower shyness scores than yours. This percentage is called a percentile. If 65% of the scores were below yours, then your score would be the 65th percentile.<br>\nA good way to calculate percentile is with linear interpolation method. It handles the rounding.<br>\n&emsp; The numbers are given ranks ranging from 1 for the lowest number to N for the highest number where N is the number of numbers. The rank R of percentile P of N numbers is: \n\n> R = P/100 x (N + 1)\n\nIf R is an integer, the Pth percentile is the number with rank R. When R is not an integer, we compute the Pth percentile by interpolation as follows:<br>\n 1. Define IR as the integer portion of R (the number to the left of the decimal point).<br>\n 2. Define FR as the fractional portion of R.<br>\n 3. Find the scores with Rank IR and with Rank IR + 1.<br>\n 4. Interpolate by multiplying the difference between the scores by FR and add the result to the lower score.<br>\n\nExample: <br>\ndata = [4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 10]<br>\nTo compute 25th percentile R = 25/100 x (20 + 1) = 21/4 = 5.25.<br>\nIR = 5 and FR = 0.25.<br>\n25th percentile = (.25) x (5 - 5) + 5 = 5.<br>\nTo compute 85th percentile R = 85/100 x (20 + 1) = 17.85.<br>\nIR = 17 and FR = 0.85<br>\n85th percentile = (0.85)(10 - 9) + 9 = 9.85<br>\n\nWe can calculate using numpy.\n\n    >>> l = [4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 10]\n    >>> np.percentile(l, 25)\n    5.0\n\nStudy more on [wiki][1].\n\n\n  [1]: https://en.wikipedia.org/wiki/Percentile#The_Linear_Interpolation_Between_Closest_Ranks_method",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "a65e59ad-cc2f-4145-8b5d-cee3a2fba72d",
        "_execution_state": "idle",
        "_uuid": "7b1413aef86a82e66636dab14961a581557f43d6"
      },
      "source": "**Central Tendency:**<br>\nImagine this situation: You are in a class with just four other students, and the five of you took a 5-point pop quiz. Today your instructor is walking around the room, handing back the quizzes. She stops at your desk and hands you your paper. Written in bold black ink on the front is \"3/5.\" How do you react? Are you happy with your score of 3 or disappointed? How do you decide? You might calculate your percentage correct, realize it is 60%, and be appalled. But it is more likely that when deciding how to react to your performance, you will want additional information. What additional information would you like?\nIf you are like most students, you will immediately ask your neighbors, \"Whad'ja get?\" and then ask the instructor, \"How did the class do?\" In other words, the additional information you want is how your quiz score compares to other students' scores. You therefore understand the importance of comparing your score to the class distribution of scores. Should your score of 3 turn out to be among the higher scores, then you'll be pleased after all. On the other hand, if 3 is among the lower scores in the class, you won't be quite so happy.\nThis idea of comparing individual scores to a distribution of scores is fundamental to statistics.<br>\n&emsp; Measures of central tendency are mean(arithmatic mean), median and mode.\n\n**Arithmatic Mean:**<br>\nThe arithmetic mean is the most common measure of central tendency. It is simply the sum of the numbers divided by the number of numbers. The symbol \"μ\" is used for the mean of a population. The symbol \"M\" is used for the mean of a sample. The formula for μ is shown below:\n\n    μ = ΣX/N\n    where ΣX is the sum of all the numbers in the population and\n    N is the number of numbers in the population.\n\n**Median:**<br>\nThe median is the midpoint of a distribution: the same number of scores is above the median as below it. The median can also be thought of as the 50th percentile.\n\n**Mode:**<br>\nThe mode is the most frequently occurring value.",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "0205edac-32a3-4057-b30e-f8d0374a5e10",
        "_execution_state": "idle",
        "_uuid": "1ab3632d0d73f7a496fbe67d0178219285f4a0e2",
        "trusted": false
      },
      "source": "l = [4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11]\ndf = pd.DataFrame(l, columns=['A'])\nprint(\"Mean: \", df.A.mean())\nprint(\"Median: \", df.A.median())\nprint(\"Mode: \", df.A.mode()[0])",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "d59ea8c9-a7dc-4687-97f1-0189a00e5a5e",
        "_execution_state": "idle",
        "_uuid": "d9f7021fd39cd2815cf820a2b34e8d0f3336a0db"
      },
      "source": "**Variability:**\nVariability refers to how much the numbers in a distribution differ from each other. There are four frequently used measures of variability: the range, interquartile range, variance, and standard deviation.<br>\n&emsp; The range is simply the highest score minus the lowest score. Let’s take a few examples. What is the range of the following group of numbers: 10, 2, 5, 6, 7, 3, 4? Well, the highest number is 10, and the lowest number is 2, so 10 - 2 = 8.<br>\n&emsp; The interquartile range (IQR) is the range of the middle 50% of the scores in a distribution. It is computed as follows:<br>\nIQR = 75th percentile - 25th percentile<br>\n&emsp; A related measure of variability is called the semi-interquartile range. The semi-interquartile range is defined simply as the interquartile range divided by 2. If a distribution is symmetric, the median plus or minus the semi-interquartile range contains half the scores in the distribution.<br>\n&emsp; Variability can also be defined in terms of how close the scores in the distribution are to the middle of the distribution. Using the mean as the measure of the middle of the distribution, the variance is defined as the average squared difference of the scores from the mean.<br>\n&emsp; The standard deviation is simply the square root of the variance.",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "7d57a3ba-9227-463d-8f15-0bf75ac94354",
        "_execution_state": "idle",
        "_uuid": "71f4b74dfd7df25f133c5c84aa255c19365857df",
        "trusted": false
      },
      "source": "range = df.A.max() - df.A.min()\niqr = np.percentile(l, 75) - np.percentile(l, 25)\nvariance = df.A.var()\nstd_deviation = df.A.std()\nprint(\"Range = \", range)\nprint(\"Inter quartile range = \", iqr)\nprint(\"Variance = \", variance)\nprint(\"Standard Deviation = \", std_deviation)",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "488612fa-62ce-4dc3-b17a-68aeba4da6f1",
        "_execution_state": "idle",
        "_uuid": "0d6520746d95376e4920d7ae5126cf37128159b3"
      },
      "source": "\n$$\\sum\\limits_{n=0}^{5} \\sum\\limits_{k=1}^{9} \\frac{1}{10^{n+1} k}$$",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "d11e04c4-1fd8-4ce9-af4d-defd6e83440c",
        "_execution_state": "idle",
        "_uuid": "df00fb35d84bdfa3e6b0f2fc5156b8486e9ff42d"
      },
      "source": "**Skew and Kurtosis:**<br>\nA distribution is skewed if one tail extends out further than the other. A distribution has positive skew (is skewed to the right) if the tail to the right is longer. A distribution has a negative skew (is skewed to the left) if the tail to the left is longer. Distributions with positive skew normally have larger means than medians.  The relationship between skew and the relative size of the mean and median led the statistician Pearson to propose the following simple and convenient numerical index of skew:\n$$\\frac{3(Mean-Median)}{\\sigma}$$\n&emsp; Just as there are several measures of central tendency, there is more than one measure of skew. Although Pearson's measure is a good one, the following measure is more commonly used. It is sometimes referred to as the third moment about the mean.\n$$\\sum{\\frac{(X-\\mu)^3}{\\sigma^3}}$$\nKurtosis measures how fat or thin the tails of a distribution are relative to a normal distribution. It is commonly defined as:\n$$\\sum{\\frac{(X-\\mu)^4}{\\sigma^4}} - 3$$\nThe value \"3\" is subtracted to define \"no kurtosis\" as the kurtosis of a normal distribution. Otherwise, a normal distribution would have a kurtosis of 3. Distributions with long tails are called leptokurtic; distributions with short tails are called platykurtic. Normal distributions have zero kurtosis.\n\n**Effects of linear transform on \\\\(\\mu\\\\) and \\\\(\\sigma\\\\):**<br>\nIf a variable X has a mean of \\\\(\\mu\\\\), a standard deviation of \\\\(\\sigma\\\\), and a variance of \\\\(\\sigma^2\\\\), then a new variable Y created using the linear transformation\nY = bX + A\nwill have a mean of \\\\(b\\mu+A\\\\), a standard deviation of \\\\(b\\sigma\\\\), and a variance of \\\\(b^2\\sigma^2\\\\).\n\n**Bivariate Data:**<br>\nMeasures of central tendency, variability, and spread summarize a single variable by providing important information about its distribution. Often, more than one variable is collected on each individual. For example, in large health studies of populations it is common to obtain variables such as age, sex, height, weight, blood pressure, and total cholesterol on each individual. Economic studies may be interested in, among other things, personal income and years of education. By way of illustration, let's consider something with which we are all familiar: age. Let’s begin by asking if people tend to marry other people of about the same age. Our experience tells us \"yes,\" but how good is the correspondence? One way to address the question is to look at pairs of ages for a sample of married couples.  Lets consider following data of couple's age.",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "05fab3f0-1ac7-4a9a-9852-0aa6937079bc",
        "_execution_state": "idle",
        "_uuid": "c37cb87e41b6cc78201840df0aa9ed349a253374",
        "trusted": false
      },
      "source": "husband_age = [36, 72, 37, 36, 51, 50, 47, 50, 37, 41]\nwife_age = [35, 67, 33, 35, 50, 46, 47, 42, 36, 41]\ndata = pd.DataFrame(list(zip(husband_age, wife_age)), columns=['HusbandAge', 'WifeAge'])\nprint(data)\nprint(\"Husband Age mean: \", data.HusbandAge.mean())\nprint(\"Wife Age mean: \", data.WifeAge.mean())\nplt.scatter(data.HusbandAge, data.WifeAge)\nplt.xlabel(\"Husband Age\")\nplt.ylabel(\"Wife Age\")",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "4a03ecc3-6a7c-4895-9f93-45528d1a5103",
        "_execution_state": "idle",
        "_uuid": "647e2821fc8b2fbd09ec79d4e4b7b841b4e14f35"
      },
      "source": "Even though we provide summary statistics on each variable, the pairing within couple is lost by separating the variables. We cannot say, for example, based on the means alone what percentage of couples has younger husbands than wives. Only by maintaining the pairing can meaningful answers be found about couples per se. We can learn much more by displaying the bivariate data in a graphical form that maintains the pairing. There are two important characteristics of the data revealed by above scatter plot. First, it is clear that there is a strong relationship between the husband's age and the wife's age: the older the husband, the older the wife. When one variable (Y) increases with the second variable (X), we say that X and Y have a positive association. Conversely, when Y decreases as X increases, we say that they have a negative association. Second, the points cluster along a straight line. When this occurs, the relationship is called a linear relationship. Scatter plots that show linear relationships between variables can differ in several ways including the slope of the line about which they cluster and how tightly the points cluster about the line. A statistical measure of the strength of the relationship between two quantitative variables that takes these factors into account is Pearson's Correlation Coefficient.<br>\nIf the relationship between the variables is not linear, then the correlation coefficient does not adequately represent the strength of the relationship between the variables. The symbol for Pearson's correlation is \"ρ\" when it is measured in the population and \"r\" when it is measured in a sample. Because we will be dealing almost exclusively with samples, we will use r to represent Pearson's correlation unless otherwise noted. Pearson's r can range from -1 to 1. An r of -1 indicates a perfect negative linear relationship between variables, an r of 0 indicates no linear relationship between variables, and an r of 1 indicates a perfect positive linear relationship between variables.<br>\nA critical property of Pearson's r is that it is unaffected by linear transformations. This means that multiplying a variable by a constant and/or adding a constant does not change the correlation of that variable with other variables. For instance, the correlation of Weight and Height does not depend on whether Height is measured in inches, feet, or even miles. Similarly, adding five points to every student's test score would not change the correlation of the test score with other variables such as GPA.<br>\nTo compute Pearson's r we begin by computing the mean for X and subtracting this mean from all values of X. The new variable is called \"x.\" The variable \"y\" is computed similarly. The variables x and y are said to be deviation scores because each score is a deviation from the mean. Notice that the means of x and y are both 0. Next we create a new column by multiplying x and y. Pearson's r is designed so that the correlation between height and weight is the same whether height is measured in inches or in feet. To achieve this property, Pearson's correlation is computed by dividing the sum of the xy column (Σxy) by the square root of the product of the sum of the  \\\\(x^2\\\\)  column  \\\\(\\sum{x^2}\\\\) and the sum of the  \\\\(y^2\\\\)  column \\\\(\\sum{y^2}\\\\). The resulting formula is:\n$$\\frac{\\sum{xy}}{\\sqrt{\\sum{x2}\\sum{y2}}}$$\n\n \n",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "a8c07f00-4526-424f-bfd7-0f52feee8617",
        "_execution_state": "idle",
        "_uuid": "b9364d13f883dcc934e6da08afdfe073fb43db47",
        "trusted": false
      },
      "source": "data['x'] = data.HusbandAge - data.HusbandAge.mean()\ndata['y'] = data.WifeAge - data.WifeAge.mean()\ndata['xy'] = data.x * data.y\nr = np.sum(data.xy)/((np.sum(data.x**2) * np.sum(data.y**2)) **.5)\n# Pearson's correlation coefficient for above spousal data\nr",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "bb9e27ff-f1ed-405a-910a-de3de9b7a229",
        "_execution_state": "idle",
        "_uuid": "5ddf39a2514fdab7572ef1623fed906b4b94678c"
      },
      "source": "**Probability:**<br>\nInferential statistics is built on the foundation of probability theory, and has been remarkably successful in guiding opinion about the conclusions to be drawn from data. Yet (paradoxically) the very idea of probability has been plagued by controversy from the beginning of the subject to the present day. In this section we provide a glimpse of the debate about the interpretation of the probability concept.<br>\n&emsp; One conception of probability is drawn from the idea of symmetrical outcomes. For example, the two possible outcomes of tossing a fair coin seem not to be distinguishable in any way that affects which side will land up or down. Therefore the probability of heads is taken to be 1/2, as is the probability of tails. In general, if there are N symmetrical outcomes, the probability of any given one of them occurring is taken to be 1/N. Thus, if a six-sided die is rolled, the probability of any one of the six sides coming up is 1/6.<br>\n&emsp; Probabilities can also be thought of in terms of relative frequencies. If we tossed a coin millions of times, we would expect the proportion of tosses that came up heads to be pretty close to 1/2. As the number of tosses increases, the proportion of heads approaches 1/2. Therefore, we can say that the probability of a head is 1/2.<br>\n&emsp; If it has rained in Seattle on 62% of the last 100,000 days, then the probability of it raining tomorrow might be taken to be 0.62. This is a natural idea but nonetheless unreasonable if we have further information relevant to whether it will rain tomorrow. For example, if tomorrow is August 1, a day of the year on which it seldom rains in Seattle, we should only consider the percentage of the time it rained on August 1. But even this is not enough since the probability of rain on the next August 1 depends on the humidity. (The chances are higher in the presence of high humidity.) So, we should consult only the prior occurrences of August 1 that had the same humidity as the next occurrence of August 1. Of course, wind direction also affects probability ... You can see that our sample of prior cases will soon be reduced to the empty set. Anyway, past meteorological history is misleading if the climate is changing.<br>\nFor some purposes, probability is best thought of as subjective. Questions such as \"What is the probability that Ms. Garcia will defeat Mr. Smith in an upcoming congressional election?\" do not conveniently fit into either the symmetry or frequency approaches to probability. Rather, assigning probability 0.7 (say) to this event seems to reflect the speaker's personal opinion --- perhaps his willingness to bet according to certain odds. Such an approach to probability, however, seems to lose the objective content of the idea of chance; probability becomes mere opinion.<br>\n&emsp; Two people might attach different probabilities to the election outcome, yet there would be no criterion for calling one \"right\" and the other \"wrong.\" We cannot call one of the two people right simply because she assigned higher probability to the outcome that actually transpires. After all, you would be right to attribute probability 1/6 to throwing a six with a fair die, and your friend who attributes 2/3 to this event would be wrong. And you are still right (and your friend is still wrong) even if the die ends up showing a six! The lack of objective criteria for adjudicating claims about probabilities in the subjective perspective is an unattractive feature of it for many scholars.<>\nLike most work in the field, the present text adopts the frequentist approach to probability in most cases. Moreover, almost all the probabilities we shall encounter will be nondogmatic, that is, neither zero nor one. An event with probability 0 has no chance of occurring; an event of probability 1 is certain to occur. It is hard to think of any examples of interest to statistics in which the probability is either 0 or 1. (Even the probability that the Sun will come up tomorrow is less than 1.)<br>\n&emsp;The following example illustrates our attitude about probabilities. Suppose you wish to know what the weather will be like next Saturday because you are planning a picnic. You turn on your radio, and the weather person says, “There is a 10% chance of rain.” You decide to have the picnic outdoors and, lo and behold, it rains. You are furious with the weather person. But was she wrong? No, she did not say it would not rain, only that rain was unlikely. She would have been flatly wrong only if she said that the probability is 0 and it subsequently rained. However, if you kept track of her weather predictions over a long period of time and found that it rained on 50% of the days that the weather person said the probability was 0.10, you could say her probability assessments are wrong. So when is it accurate to say that the probability of rain is 0.10? According to our frequency interpretation, it means that it will rain 10% of the days on which rain is forecast with this probability.\n$$probability = \\frac{number\\ of\\ favorable\\ outcome}{number\\ of\\ possible\\ equally-likely\\ outcome}$$\nFor example, what is the probability that a card drawn at random from a deck of playing cards will be an ace? Since the deck has four aces, there are four favorable outcomes; since the deck has 52 cards, there are 52 possible outcomes. The probability is therefore 4/52 = 1/13. What about the probability that the card will be a club? Since there are 13 clubs, the probability is 13/52 = 1/4.<br>\n&emsp; Events A and B are independent events if the probability of Event B occurring is the same whether or not Event A occurs. Let's take a simple example. A fair coin is tossed two times. The probability that a head comes up on the second toss is 1/2 regardless of whether or not a head came up on the first toss. The two events are (1) first toss is a head and (2) second toss is a head. So these events are independent. Consider the two events (1) \"It will rain tomorrow in Houston\" and (2) \"It will rain tomorrow in Galveston\" (a city near Houston). These events are not independent because it is more likely that it will rain in Galveston on days it rains in Houston than on days it does not.<br>\n&emsp; When two events are independent, the probability of both occurring is the product of the probabilities of the individual events. More formally, if events A and B are independent, then the probability of both A and B occurring is:\n**P(A and B)** = P(A) x P(B) where P(A and B) is the probability of events A and B both occurring, P(A) is the probability of event A occurring, and P(B) is the probability of event B occurring.<br>\n&emsp; If you flip a coin twice, what is the probability that it will come up heads both times? Event A is that the coin comes up heads on the first flip and Event B is that the coin comes up heads on the second flip. Since both P(A) and P(B) equal 1/2, the probability that both events occur is 1/2 x 1/2 = 1/4.<br>\nIf Events A and B are independent, the probability that either Event A or Event B occurs is: **P(A or B)** = P(A) + P(B) - P(A and B)<br>\n&emsp; If you flip a coin two times, what is the probability that you will get a head on the first flip or a head on the second flip (or both)? Letting Event A be a head on the first flip and Event B be a head on the second flip, then P(A) = 1/2, P(B) = 1/2, and P(A and B) = 1/4. Therefore, P(A or B) = 1/2 + 1/2 - 1/4 = 3/4.<br>\n&emsp; Often it is required to compute the probability of an event given that another event has occurred. For example, what is the probability that two cards drawn at random from a deck of playing cards will both be aces? It might seem that you could use the formula for the probability of two independent events and simply multiply 4/52 x 4/52 = 1/169. This would be incorrect, however, because the two events are not independent. If the first card drawn is an ace, then the probability that the second card is also an ace would be lower because there would only be three aces left in the deck. Once the first card chosen is an ace, the probability that the second card chosen is also an ace is called the conditional probability of drawing an ace. In this case, the \"condition\" is that the first card is an ace. Symbolically, we write this as: P(ace on second draw | an ace on the first draw). The vertical bar \"|\" is read as \"given,\" so the above expression is short for: \"The probability that an ace is drawn on the second draw given that an ace was drawn on the first draw.\" What is this probability? Since after an ace is drawn on the first draw, there are 3 aces out of 51 total cards left. This means that the probability that one of these aces will be drawn is 3/51 = 1/17.\nIf Events A and B are not independent, then P(A and B) = P(A) x P(B|A).<br>\n&emsp; Applying this to the problem of two aces, the probability of drawing two aces from a deck is 4/52 x 3/51 = 1/221.",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "a693a79d-eeab-4f22-930a-179a56f6881a",
        "_execution_state": "idle",
        "_uuid": "69ac6ad8298a05285ed66464ad03890a93fcd9ec"
      },
      "source": "Possible Orders:<br>\nSuppose you had a plate with three pieces of candy on it: one green, one yellow, and one red. You are going to pick up these three pieces one at a time. The question is: In how many different orders can you pick up the pieces? There are two orders in which red is first: red, yellow, green and red, green, yellow. Similarly, there are two orders in which yellow is first and two orders in which green is first. This makes six possible orders in which the pieces can be picked up.<br>\nNumber\tFirst\tSecond\tThird<br>\n1\t     red\t        yellow\tgreen<br>\n2\t     red\t        green\tyellow<br>\n3\t     yellow\tred\t        green<br>\n4\t     yellow\tgreen\tred<br>\n5\t     green\t        red\t        yellow<br>\n6\t     green\t        yellow\tred<br>\n\nNumber of orders = n!\n\nMultiplication Rule:<br>\nImagine a small restaurant whose menu has 3 soups, 6 entrées, and 4 desserts. How many possible meals are there? The answer is calculated by multiplying the numbers to get 3 x 6 x 4 = 72. You can think of it as first there is a choice among 3 soups. Then, for each of these choices there is a choice among 6 entrées resulting in 3 x 6 = 18 possibilities. Then, for each of these 18 possibilities there are 4 possible desserts yielding 18 x 4 = 72 total possibilities.<br>\n\nPermutations:<br>\nSuppose that there were four pieces of candy (red, yellow, green, and brown) and you were only going to pick up exactly two pieces. How many ways are there of picking up two pieces? The first choice can be any of the four colors. For each of these 4 first choices there are 3 second choices. Therefore there are 4 x 3 = 12 possibilities.\nNumber\tFirst\tSecond\n1\t          red\tyellow\n2\t          red\tgreen\n3\t          red\tbrown\n4\t     yellow\tred\n5\t     yellow\tgreen\n6\t     yellow\tbrown\n7\t      green\tred\n8\t      green\tyellow\n9\t      green\tbrown\n10\t     brown\tred\n11\t     brown\tyellow\n12\t     brown\tgreen\n$$_{n} P_{r}=\\frac{n!}{(n-r)!}$$\nIt is important to note that order counts in permutations. That is, choosing red and then yellow is counted separately from choosing yellow and then red. Therefore permutations refer to the number of ways of choosing rather than the number of possible outcomes. When order of choice is not considered, the formula for combinations is used.\n\n\nCombinations:<br>\nNow suppose that you were not concerned with the way the pieces of candy were chosen but only in the final choices. In other words, how many different combinations of two pieces could you end up with? In counting combinations, choosing red and then yellow is the same as choosing yellow and then red because in both cases you end up with one red piece and one yellow piece. Unlike permutations, order does not count.\n$$_{n} C_{r}=\\frac{n!}{(n-r)!r!}$$",
      "outputs": [],
      "cell_type": "markdown",
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "e2d4385d-b8f6-4c84-834e-8b1637981f55",
        "_execution_state": "idle",
        "_uuid": "7807be21f943b86fadf7a6948d60d3d32f7a7b02",
        "trusted": false
      },
      "source": "df = pd.read_csv(\"data/Advertising.csv\")\nplt.scatter(df.TV, df.Sales)",
      "outputs": [],
      "cell_type": "code",
      "execution_count": null
    }
  ],
  "nbformat_minor": 0,
  "nbformat": 4
}
